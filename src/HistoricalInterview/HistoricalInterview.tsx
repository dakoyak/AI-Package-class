import { useEffect, useRef, useState } from 'react';
import './HistoricalInterview.css';
import SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';

const API_BASE_URL = import.meta.env.VITE_API_URL ?? 'http://localhost:5001';

type HistoricalInterviewProps = {
  onStartLesson: () => void;
  onEndLesson: () => void;
};

type ChatMessage = {
  id: number;
  sender: 'user' | 'ai';
  text: string;
};

type SejongResponse = {
  answer: string;
};

function HistoricalInterview({ onStartLesson, onEndLesson }: HistoricalInterviewProps) {
  const [isStarted, setIsStarted] = useState(false);
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [isAiThinking, setIsAiThinking] = useState(false);
  const chatLogEndRef = useRef<HTMLDivElement | null>(null);

  const { transcript, listening, resetTranscript, browserSupportsSpeechRecognition } = useSpeechRecognition();

  const speakText = (text: string) => {
    const speech = new SpeechSynthesisUtterance(text);
    const setKoreanVoice = () => {
      const voices = window.speechSynthesis.getVoices();
      const koreanMaleVoice = voices.find((voice) => voice.lang === 'ko-KR' && voice.name.includes('ë‚¨ì„±'));
      const anyKoreanVoice = voices.find((voice) => voice.lang === 'ko-KR');

      speech.voice = koreanMaleVoice || anyKoreanVoice || null;
      speech.lang = 'ko-KR';
      speech.pitch = 0.9;
      speech.rate = 1.0;
    };
    setKoreanVoice();
    if (window.speechSynthesis.onvoiceschanged !== undefined) {
      window.speechSynthesis.onvoiceschanged = setKoreanVoice;
    }
    window.speechSynthesis.cancel();
    window.speechSynthesis.speak(speech);
  };

  const getSejongResponse = async (userQuestion: string) => {
    setIsAiThinking(true);
    setMessages((prev) => [...prev, { id: Date.now(), sender: 'user', text: userQuestion }]);

    try {
      const response = await fetch(`${API_BASE_URL}/api/ask-sejong`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ question: userQuestion }),
      });
      const data: SejongResponse = await response.json();

      setMessages((prev) => [...prev, { id: Date.now() + 1, sender: 'ai', text: data.answer }]);
      speakText(data.answer);
    } catch {
      const errorText = 'ë¯¸ì•ˆí•˜êµ¬ë‚˜, ì§ì´ ì§€ê¸ˆ ìƒê°ì´ ë§ìœ¼ë‹ˆë¼.';
      setMessages((prev) => [...prev, { id: Date.now() + 1, sender: 'ai', text: errorText }]);
      speakText(errorText);
    } finally {
      setIsAiThinking(false);
    }
  };

  const handleStartListening = () => {
    SpeechRecognition.startListening({ continuous: false, language: 'ko-KR' });
  };

  useEffect(() => {
    if (!listening && transcript) {
      getSejongResponse(transcript);
      resetTranscript();
    }
  }, [listening, transcript, resetTranscript]);

  useEffect(() => {
    chatLogEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  const handleStart = () => {
    onStartLesson();
    setIsStarted(true);
    const initialMessage = 'ë‚´ê°€ ì„¸ì¢…ëŒ€ì™•ì´ë‹¤. ì§ì—ê²Œ ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€?';
    setMessages([{ id: 1, sender: 'ai', text: initialMessage }]);
    setTimeout(() => speakText(initialMessage), 500);
  };

  const handleEnd = () => {
    window.speechSynthesis.cancel();
    SpeechRecognition.abortListening();
    onEndLesson();
    setIsStarted(false);
  };

  if (!browserSupportsSpeechRecognition) {
    return (
      <div className="lesson-start-screen">
        <div className="start-screen-content">
          <h1>ì˜¤ë¥˜</h1>
          <p>ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Chrome ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.</p>
        </div>
      </div>
    );
  }

  if (!isStarted) {
    return (
      <div className="lesson-start-screen">
        <div className="start-screen-content">
          <div className="start-screen-icon">ğŸ‘‘</div>
          <h1>AI ì—­ì‚¬ ì¸í„°ë·°</h1>
          <p>AI ì„¸ì¢…ëŒ€ì™•ê³¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëŒ€í™”í•˜ë©° ì—­ì‚¬ë¥¼ ë°°ì›Œë³´ì„¸ìš”.</p>
          <button className="start-lesson-btn" onClick={handleStart}>
            ìˆ˜ì—… ì‹œì‘í•˜ê¸°
          </button>
        </div>
      </div>
    );
  }

  return (
    <div className="interview-page-container">
      <button className="exit-lesson-btn" onClick={handleEnd}>
        &times; ë©”ë‰´ë¡œ ëŒì•„ê°€ê¸°
      </button>
      <div className="interview-character-zone">
        <div className="character-image-wrapper">
          <img src="/sejong.jpg" alt="AI ì„¸ì¢…ëŒ€ì™•" className={isAiThinking ? 'thinking' : ''} />
        </div>
        <h3 className="character-name">AI ì„¸ì¢…ëŒ€ì™•</h3>
        <p className="character-status">
          {listening ? 'ë“£ê³  ìˆë…¸ë¼...' : isAiThinking ? 'ìƒê° ì¤‘ì´ë‹ˆë¼...' : 'ëŒ€í™” ê°€ëŠ¥'}
        </p>
      </div>
      <div className="interview-chat-log">
        {messages.map((msg) => (
          <div key={msg.id} className={`message-bubble ${msg.sender}`}>
            <div className="message-avatar">
              {msg.sender === 'ai' ? <img src="/sejong.jpg" alt="AI ì„¸ì¢…ëŒ€ì™• ì•„ë°”íƒ€" /> : 'ğŸ“'}
            </div>
            <div className="message-text">{msg.text}</div>
          </div>
        ))}
        {listening && (
          <div className="message-bubble user">
            <div className="message-avatar">ğŸ“</div>
            <div className="message-text transcript">{transcript}</div>
          </div>
        )}
        {isAiThinking && (
          <div className="message-bubble ai">
            <div className="message-text loading-dots">
              <span>.</span>
              <span>.</span>
              <span>.</span>
            </div>
          </div>
        )}
        <div ref={chatLogEndRef} />
      </div>
      <footer className="interview-input-area">
        <button className={`mic-btn ${listening ? 'listening' : ''}`} onClick={handleStartListening} disabled={isAiThinking}>
          ğŸ¤
          <span>{listening ? 'ë“£ëŠ” ì¤‘...' : 'ëˆŒëŸ¬ì„œ ì§ˆë¬¸í•˜ê¸°'}</span>
        </button>
      </footer>
    </div>
  );
}

export default HistoricalInterview;
